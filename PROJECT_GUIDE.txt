PROJECT GUIDE – Speech Emotion Recognition (SER) using ML and DL

1) Overview
- Goal: Classify emotions from speech audio into 8 classes: [neutral, calm, happy, sad, angry, fear, disgust, surprised].
- Datasets: RAVDESS (primary) and TESS (optional, to augment with two additional actors).
- Outputs:
  - Precomputed features: dataset_features/X.joblib and dataset_features/y.joblib
  - Pretrained deep learning model: Deep Learning/SER_model.h5
  - Example inference predictions from WAV files under examples/

2) Environment Setup (macOS, Apple Silicon)
- Create & activate virtual environment:
  - python3 -m venv .venv
  - source .venv/bin/activate
- Install dependencies:
  - pip install --upgrade pip setuptools wheel
  - pip install -r requirements.txt
  - pip install --no-cache-dir tensorflow-macos tensorflow-metal
- Confirm imports:
  - python -c "import librosa, tensorflow as tf; print(librosa.__version__, tf.__version__)"

3) Repository Structure (key paths)
- config.py: Cross-platform paths and helpers (auto-creates feature folders).
- tess_pipeline.py: Converts TESS files into RAVDESS-like naming; fills features/Actor_25 and features/Actor_26.
- create_features.py: Walks features/ (Actor_01..Actor_26) to compute MFCC features and saves them as joblib files.
- Deep Learning/SER(Deep_Learning).ipynb: Training notebook (CNN over MFCCs).
- Deep Learning/SER_model.h5: Pretrained model weights (legacy Keras H5; weights-only).
- run_inference.py: CLI script to run predictions on WAV files using the pretrained model.

4) Datasets
- RAVDESS: Download Speech and Song sets; merge by Actor index so that features/Actor_01 .. features/Actor_24 exist.
- TESS (optional): Download and unzip to TESS_Toronto_emotional_speech_set_data/.
  - This directory should contain subfolders like OAF_angry, OAF_disgust, … and YAF_…

5) Augmenting with TESS (optional)
- What it does: Copies TESS WAV files into features/Actor_25 (YAF) and features/Actor_26 (OAF) using a naming convention compatible with RAVDESS.
- How to run:
  - Ensure TESS is placed at: TESS_Toronto_emotional_speech_set_data/
  - python tess_pipeline.py
- Result:
  - New WAV files copied into features/Actor_25 and features/Actor_26 with RAVDESS-like names.

6) Feature Creation
- What it does: Extracts MFCC features (40 coefficients, mean across time) from all WAVs under features/Actor_*/.
- How to run:
  - python create_features.py
- Result:
  - Saves dataset_features/X.joblib (features) and dataset_features/y.joblib (labels 0..7).
- Notes:
  - Precomputed features already exist in dataset_features/. You only need to run this if you changed/added audio files.

7) Model Training (optional)
- Where: Deep Learning/SER(Deep_Learning).ipynb
- Summary:
  - Loads features X, y
  - Builds a Conv1D model with input shape (40,1)
  - Trains and evaluates, then saves model to .h5
- Output:
  - A trained model (weights). In this repo, Deep Learning/SER_model.h5 is provided.

8) Inference (end-to-end test)
- Script: run_inference.py
- What it does:
  - Loads the pretrained model (Deep Learning/SER_model.h5). If direct load fails (legacy format), it rebuilds the original architecture and loads weights by name.
  - Extracts MFCCs from the provided WAV file and runs prediction.
- How to run:
  - python run_inference.py --model "Deep Learning/SER_model.h5" --file examples/10-16-07-29-82-30-63.wav
  - python run_inference.py --model "Deep Learning/SER_model.h5" --file examples/03-01-01-01-01-02-05.wav
- Output (example):
  - {'file': '...wav', 'pred_index': 6, 'pred_label': 'disgust', 'confidence': 0.9998}

9) Labels
- Index to label mapping used by the project:
  - 0 neutral, 1 calm, 2 happy, 3 sad, 4 angry, 5 fear, 6 disgust, 7 surprised

10) What happens under the hood
- tess_pipeline.py:
  - Scans TESS directories, maps label names (e.g., "angry", "disgust") to the RAVDESS numeric code in the filename, assigns synthetic values for unused parts of the name, and copies into Actor_25 (YAF) / Actor_26 (OAF).
- create_features.py:
  - For each WAV: loads audio with librosa, computes 40 MFCC coefficients, averages across time, derives the label from the RAVDESS-like filename, and appends to datasets X (float32) and y (int).
  - Dumps X.joblib and y.joblib for fast reuse.
- run_inference.py:
  - Attempts to load the legacy .h5 directly. If not compatible with your TensorFlow/Keras version, recreates the Conv1D architecture and loads weights by name, then performs prediction on MFCC features extracted from your WAV.

11) Common issues & fixes
- TensorFlow on Apple Silicon (Python 3.12): Install tensorflow-macos and tensorflow-metal; ensure you are in a venv.
- librosa resampling error: Install resampy (pip install resampy) or use res_type=None when loading.
- H5 model load error: Use run_inference.py which includes a fallback loader.

12) Expected end results
- With the provided example WAVs, the model should output emotion predictions with a confidence score.
- You can replace the input WAV with any audio clip (mono recommended) to get a predicted emotion.

13) Repro tips
- Keep audio sampling rates default; the pipeline resamples as needed.
- If you add many new files, re-run create_features.py and retrain via the notebook for best accuracy.
